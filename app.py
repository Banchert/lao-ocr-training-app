# app.py (‡∏â‡∏ö‡∏±‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡∏°‡πà)
import os
import json
import threading
import subprocess
import time
import sys
from flask import Flask, render_template, request, jsonify, Response, send_file, send_from_directory
from werkzeug.utils import secure_filename
import logging

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# üì¶ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á Flask
app = Flask(__name__,
            template_folder='templates',
            static_folder='static')

# ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå 16MB
ALLOWED_FONT_EXTENSIONS = {'.ttf', '.otf'}
ALLOWED_TEXT_EXTENSIONS = {'.txt'}

# üóÇÔ∏è ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
BASE_FOLDERS = ['fonts', 'corpus', 'dataset', 'models', 'training_scripts', 'logs']
for folder in BASE_FOLDERS:
    os.makedirs(folder, exist_ok=True)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå static ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
os.makedirs('static/css', exist_ok=True)
os.makedirs('static/js', exist_ok=True)
os.makedirs('templates', exist_ok=True)

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
def check_required_files():
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏£‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
    required_files = {
        'training_scripts/generate_dataset.py': '‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏£‡πâ‡∏≤‡∏á dataset',
        'training_scripts/train_ocr.py': '‡πÑ‡∏ü‡∏•‡πå‡∏ù‡∏∂‡∏Å‡∏™‡∏≠‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•'
    }
    
    missing_files = []
    for filepath, description in required_files.items():
        if not os.path.exists(filepath):
            missing_files.append(f"{description} ({filepath})")
    
    return missing_files

# üìÑ ‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å
@app.route('/')
def index():
    missing_files = check_required_files()
    if missing_files:
        logger.warning(f"Missing files: {missing_files}")
    return render_template('index.html')

# üì§ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏ü‡∏≠‡∏ô‡∏ï‡πå (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
@app.route('/upload/font', methods=['POST'])
def upload_font():
    try:
        if 'file' not in request.files:
            return jsonify({"success": False, "error": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå"}), 400
        
        files = request.files.getlist('file')
        uploaded_files = []
        
        for file in files:
            if file and file.filename:
                filename = secure_filename(file.filename)
                file_ext = os.path.splitext(filename)[1].lower()
                
                if file_ext not in ALLOWED_FONT_EXTENSIONS:
                    continue
                
                filepath = os.path.join('fonts', filename)
                file.save(filepath)
                uploaded_files.append(filename)
                logger.info(f"Uploaded font: {filename}")
        
        if uploaded_files:
            return jsonify({
                "success": True, 
                "files": uploaded_files,
                "message": f"‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à {len(uploaded_files)} ‡πÑ‡∏ü‡∏•‡πå"
            })
        else:
            return jsonify({"success": False, "error": "‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á"}), 400
            
    except Exception as e:
        logger.error(f"Upload error: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500

# üì§ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå corpus (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
@app.route('/upload/corpus', methods=['POST'])
def upload_corpus():
    try:
        if 'file' not in request.files:
            return jsonify({"success": False, "error": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå"}), 400
        
        file = request.files['file']
        if file and file.filename:
            filename = secure_filename(file.filename)
            file_ext = os.path.splitext(filename)[1].lower()
            
            if file_ext not in ALLOWED_TEXT_EXTENSIONS:
                return jsonify({"success": False, "error": "‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå .txt"}), 400
            
            filepath = os.path.join('corpus', 'lao_corpus.txt')
            file.save(filepath)
            
            # ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
            with open(filepath, 'r', encoding='utf-8') as f:
                lines = [line.strip() for line in f if line.strip()]
            
            logger.info(f"Uploaded corpus with {len(lines)} lines")
            
            return jsonify({
                "success": True,
                "stats": {
                    "lines": len(lines),
                    "filename": filename
                }
            })
        
        return jsonify({"success": False, "error": "‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á"}), 400
        
    except Exception as e:
        logger.error(f"Corpus upload error: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500

# üìú ‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
@app.route('/list/fonts', methods=['GET'])
def list_fonts():
    try:
        files = []
        font_dir = 'fonts'
        
        for filename in os.listdir(font_dir):
            if os.path.splitext(filename)[1].lower() in ALLOWED_FONT_EXTENSIONS:
                filepath = os.path.join(font_dir, filename)
                file_size = os.path.getsize(filepath)
                files.append({
                    "name": filename,
                    "size": f"{file_size / 1024:.1f} KB"
                })
        
        return jsonify({
            "success": True, 
            "files": files,
            "total": len(files)
        })
    except Exception as e:
        logger.error(f"List fonts error: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500

# üìä ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏£‡∏∞‡∏ö‡∏ö
@app.route('/system/status', methods=['GET'])
def system_status():
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö"""
    status = {
        "ready": True,
        "checks": {}
    }
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
    missing_files = check_required_files()
    status["checks"]["required_files"] = {
        "status": len(missing_files) == 0,
        "missing": missing_files
    }
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ü‡∏≠‡∏ô‡∏ï‡πå
    font_count = len([f for f in os.listdir('fonts') 
                     if os.path.splitext(f)[1].lower() in ALLOWED_FONT_EXTENSIONS])
    status["checks"]["fonts"] = {
        "status": font_count > 0,
        "count": font_count
    }
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö corpus
    corpus_exists = os.path.exists('corpus/lao_corpus.txt')
    status["checks"]["corpus"] = {
        "status": corpus_exists,
        "exists": corpus_exists
    }
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö dataset
    dataset_count = len([f for f in os.listdir('dataset') if f.endswith('.png')])
    status["checks"]["dataset"] = {
        "status": dataset_count > 0,
        "count": dataset_count
    }
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU
    try:
        import torch
        gpu_available = torch.cuda.is_available()
        gpu_name = torch.cuda.get_device_name(0) if gpu_available else "N/A"
    except:
        gpu_available = False
        gpu_name = "N/A"
    
    status["checks"]["gpu"] = {
        "status": gpu_available,
        "available": gpu_available,
        "name": gpu_name
    }
    
    # ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
    status["ready"] = all(check["status"] for check in status["checks"].values() 
                         if "status" in check)
    
    return jsonify(status)

# ‚öôÔ∏è ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
def run_generate_process():
    """‡∏£‡∏±‡∏ô subprocess ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á dataset"""
    log_file = 'logs/dataset_log.txt'
    
    try:
        # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô log ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump({"status": "starting", "message": "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô..."}, f)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå script ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        script_path = 'training_scripts/generate_dataset.py'
        if not os.path.exists(script_path):
            raise FileNotFoundError(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {script_path}")
        
        # ‡∏£‡∏±‡∏ô subprocess
        result = subprocess.run(
            [sys.executable, script_path],
            capture_output=True,
            text=True,
            check=True
        )
        
        logger.info("Dataset generation completed successfully")
        
    except subprocess.CalledProcessError as e:
        error_msg = f"Dataset generation failed: {e.stderr}"
        logger.error(error_msg)
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump({"status": "error", "message": error_msg}, f)
    except Exception as e:
        error_msg = f"Unexpected error: {str(e)}"
        logger.error(error_msg)
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump({"status": "error", "message": error_msg}, f)

@app.route('/generate-dataset', methods=['POST'])
def generate_dataset_endpoint():
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°
    if not os.path.exists('corpus/lao_corpus.txt'):
        return jsonify({"success": False, "error": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î corpus ‡∏Å‡πà‡∏≠‡∏ô"}), 400
    
    font_count = len([f for f in os.listdir('fonts') 
                     if os.path.splitext(f)[1].lower() in ALLOWED_FONT_EXTENSIONS])
    if font_count == 0:
        return jsonify({"success": False, "error": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡∏Å‡πà‡∏≠‡∏ô"}), 400
    
    # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£
    thread = threading.Thread(target=run_generate_process)
    thread.start()
    
    return jsonify({
        "success": True, 
        "message": "‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡πâ‡∏ß",
        "fonts": font_count
    })

# üü¢ ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
@app.route('/dataset-status')
def dataset_status():
    def generate():
        log_file = 'logs/dataset_log.txt'
        last_data = None
        
        while True:
            try:
                if os.path.exists(log_file):
                    with open(log_file, 'r', encoding='utf-8') as f:
                        data = f.read()
                    
                    # ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
                    if data != last_data:
                        yield f"data: {data}\n\n"
                        last_data = data
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                    if '"status": "completed"' in data or '"status": "error"' in data:
                        break
                else:
                    yield f"data: {json.dumps({'status': 'waiting'})}\n\n"
                
                time.sleep(0.5)  # ‡∏•‡∏î delay ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ update ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
                
            except Exception as e:
                logger.error(f"Dataset status error: {str(e)}")
                yield f"data: {json.dumps({'status': 'error', 'message': str(e)})}\n\n"
                break
    
    return Response(generate(), mimetype='text/event-stream')

# ü§ñ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏™‡∏≠‡∏ô (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
def run_training_process(epochs):
    """‡∏£‡∏±‡∏ô subprocess ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å‡∏™‡∏≠‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•"""
    log_file = 'logs/training_log.txt'
    
    try:
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump({"status": "starting", "message": "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô..."}, f)
        
        script_path = 'training_scripts/train_ocr.py'
        if not os.path.exists(script_path):
            raise FileNotFoundError(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {script_path}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ dataset ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        dataset_count = len([f for f in os.listdir('dataset') if f.endswith('.png')])
        if dataset_count == 0:
            raise ValueError("‡πÑ‡∏°‡πà‡∏û‡∏ö dataset ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡∏Å‡πà‡∏≠‡∏ô")
        
        result = subprocess.run(
            [sys.executable, script_path, str(epochs)],
            capture_output=True,
            text=True,
            check=True
        )
        
        logger.info("Training completed successfully")
        
    except subprocess.CalledProcessError as e:
        error_msg = f"Training failed: {e.stderr}"
        logger.error(error_msg)
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump({"status": "error", "message": error_msg}, f)
    except Exception as e:
        error_msg = f"Unexpected error: {str(e)}"
        logger.error(error_msg)
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump({"status": "error", "message": error_msg}, f)

@app.route('/start-training', methods=['POST'])
def start_training_endpoint():
    try:
        data = request.get_json()
        epochs = int(data.get('epochs', 10))
        
        if epochs < 1 or epochs > 1000:
            return jsonify({"success": False, "error": "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô epochs ‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 1-1000"}), 400
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö dataset
        dataset_count = len([f for f in os.listdir('dataset') if f.endswith('.png')])
        if dataset_count == 0:
            return jsonify({"success": False, "error": "‡πÑ‡∏°‡πà‡∏û‡∏ö dataset ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡∏Å‡πà‡∏≠‡∏ô"}), 400
        
        thread = threading.Thread(target=run_training_process, args=(epochs,))
        thread.start()
        
        return jsonify({
            "success": True, 
            "message": "‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏™‡∏≠‡∏ô‡πÅ‡∏•‡πâ‡∏ß",
            "epochs": epochs,
            "dataset_size": dataset_count
        })
        
    except Exception as e:
        logger.error(f"Start training error: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500

# üîÅ ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ Training (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á) 
@app.route('/training-status')
def training_status():
    def generate():
        log_file = 'logs/training_log.txt'
        last_data = None
        
        while True:
            try:
                if os.path.exists(log_file):
                    with open(log_file, 'r', encoding='utf-8') as f:
                        data = f.read()
                    
                    if data != last_data:
                        yield f"data: {data}\n\n"
                        last_data = data
                    
                    if '"status": "completed"' in data or '"status": "error"' in data:
                        break
                else:
                    yield f"data: {json.dumps({'status': 'idle'})}\n\n"
                
                time.sleep(0.5)
                
            except Exception as e:
                logger.error(f"Training status error: {str(e)}")
                yield f"data: {json.dumps({'status': 'error', 'message': str(e)})}\n\n"
                break
    
    return Response(generate(), mimetype='text/event-stream')

# üñºÔ∏è ‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô dataset
@app.route('/dataset/preview/<int:page>')
def dataset_preview(page=1):
    """‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô dataset"""
    try:
        per_page = 20
        image_files = [f for f in os.listdir('dataset') if f.endswith('.png')]
        image_files.sort()
        
        total_images = len(image_files)
        total_pages = (total_images + per_page - 1) // per_page
        
        start_idx = (page - 1) * per_page
        end_idx = min(start_idx + per_page, total_images)
        
        images = []
        for i in range(start_idx, end_idx):
            img_name = image_files[i]
            txt_file = img_name.replace('.png', '.gt.txt')
            
            text = ""
            if os.path.exists(os.path.join('dataset', txt_file)):
                with open(os.path.join('dataset', txt_file), 'r', encoding='utf-8') as f:
                    text = f.read().strip()
            
            images.append({
                "filename": img_name,
                "text": text
            })
        
        return jsonify({
            "success": True,
            "images": images,
            "page": page,
            "total_pages": total_pages,
            "total_images": total_images
        })
        
    except Exception as e:
        logger.error(f"Dataset preview error: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500

# üñºÔ∏è ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
@app.route('/dataset/image/<filename>')
def dataset_image(filename):
    """‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å dataset"""
    try:
        return send_from_directory('dataset', filename)
    except Exception as e:
        logger.error(f"Image serving error: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 404

# üì• ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
@app.route('/models/list')
def list_models():
    """‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡πâ‡∏ß"""
    try:
        models = []
        for filename in os.listdir('models'):
            if filename.endswith('.pth'):
                filepath = os.path.join('models', filename)
                file_size = os.path.getsize(filepath)
                file_time = os.path.getmtime(filepath)
                
                models.append({
                    "filename": filename,
                    "size": f"{file_size / 1024 / 1024:.2f} MB",
                    "created": time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(file_time))
                })
        
        models.sort(key=lambda x: x["created"], reverse=True)
        
        return jsonify({
            "success": True,
            "models": models
        })
        
    except Exception as e:
        logger.error(f"List models error: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/models/download/<filename>')
def download_model(filename):
    """‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•"""
    try:
        return send_file(
            os.path.join('models', filename),
            as_attachment=True,
            download_name=filename
        )
    except Exception as e:
        logger.error(f"Model download error: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 404

# üßπ ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
@app.route('/clear/<data_type>', methods=['POST'])
def clear_data(data_type):
    """‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó"""
    try:
        if data_type == 'dataset':
            for f in os.listdir('dataset'):
                os.remove(os.path.join('dataset', f))
            message = "‡∏•‡∏ö dataset ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÅ‡∏•‡πâ‡∏ß"
        elif data_type == 'models':
            for f in os.listdir('models'):
                if f.endswith('.pth'):
                    os.remove(os.path.join('models', f))
            message = "‡∏•‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÅ‡∏•‡πâ‡∏ß"
        elif data_type == 'fonts':
            for f in os.listdir('fonts'):
                if os.path.splitext(f)[1].lower() in ALLOWED_FONT_EXTENSIONS:
                    os.remove(os.path.join('fonts', f))
            message = "‡∏•‡∏ö‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÅ‡∏•‡πâ‡∏ß"
        else:
            return jsonify({"success": False, "error": "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á"}), 400
        
        logger.info(f"Cleared {data_type}")
        return jsonify({"success": True, "message": message})
        
    except Exception as e:
        logger.error(f"Clear data error: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500

# üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏° Flask App
if __name__ == '__main__':
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°
    missing_files = check_required_files()
    if missing_files:
        logger.warning("=== ‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô: ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö ===")
        for file in missing_files:
            logger.warning(f"- {file}")
        logger.warning("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå log ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
    os.makedirs('logs', exist_ok=True)
    for log_file in ['logs/dataset_log.txt', 'logs/training_log.txt']:
        if not os.path.exists(log_file):
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump({"status": "idle"}, f)
    
    app.run(debug=True, threaded=True, host='0.0.0.0', port=5000)